{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pushkin_stihi",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7_e3GBMPX_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e348314e-19c9-48a6-ae75-d65ba9ba9083"
      },
      "source": [
        "!pip install transformers==4.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.2.2\n",
            "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 39.0 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 184 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 215 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 245 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 276 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 307 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 337 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 368 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 389 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 399 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 430 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 440 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 460 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 491 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 522 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 583 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 614 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 645 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 675 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 706 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 737 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 768 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 788 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 798 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 819 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 849 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 860 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 880 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 890 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 911 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 921 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 952 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 972 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 983 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.0 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.3 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.6 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.7 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.6.4)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH0JtdCeE1n2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcb8d84-48fe-4a7d-b5e7-45f3193cfdc7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 14 08:43:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W /  70W |  15090MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH1W6YrdjNSp"
      },
      "source": [
        "# Prepare the dataset and build a ``TextDataset``\n",
        "\n",
        "The next step is to extract the instructions from all recipes and build a `TextDataset`. The `TextDataset` is a custom implementation of the [Pytroch `Dataset` class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) implemented by the transformers library. If you want to know more about Dataset in Pytroch you can check out this [youtube video](https://www.youtube.com/watch?v=PXOzkkB5eH0&ab_channel=PythonEngineer).\n",
        "\n",
        "First, we are going to split the `recipes.json` into a `train` and `test` section and extract `Instructions` from the recipes and write them into a `train_dataset.txt` and `test_dataset.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "813dmP6nrmI9"
      },
      "source": [
        "with open('/content/стищки.txt','r',encoding='windows-1251') as f:\n",
        "    lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WejlSHLnv3oc"
      },
      "source": [
        "for line in lines:\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AsAz3PuvcoG"
      },
      "source": [
        "fl = open('test.txt', 'w')\n",
        "fl.writelines(lines)\n",
        "fl.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAdk8mQuNVB"
      },
      "source": [
        "with open('test.txt','r') as f:\n",
        "    lines = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V36gOIOfLHvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3e7c42-87dc-4789-b0b4-ab1dcf910b69"
      },
      "source": [
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "with open('/content/test.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "def build_text_files(lines, dest_path):\n",
        "    \"\"\"\n",
        "    name:name \\n\n",
        "    poem:start\n",
        "    poem \\n\n",
        "    poem:end\n",
        "\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for i in range(len(lines)):\n",
        "      line = lines[i]\n",
        "      if not line.startswith('\\t\\t') and len(line)>4 and line.find('* * *')!=0:\n",
        "          data+=['poem:end' + '\\n' +'name:'+(line)+'\\n' + 'poem:start']\n",
        "          \n",
        "      else:\n",
        "        \n",
        "        data+=[line.replace('–\\xa0','').replace('»','').replace('«','')]\n",
        "      \n",
        "    \n",
        "    \n",
        "    fl = open(dest_path, 'w')\n",
        "    fl.writelines(data)\n",
        "    fl.close()\n",
        "\n",
        "train, test = train_test_split(lines,test_size=0.1) \n",
        "\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n",
        "\n",
        "print(\"Train dataset length: \"+str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset length: 23911\n",
            "Test dataset length: 2657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg78wNnTl305"
      },
      "source": [
        "the next step is to download the tokenizer, which we use. We use the tokenizer from the `german-gpt2` model on [huggingface](https://huggingface.co/anonymous-german-nlp/german-gpt2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCaunLMtlPfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452a63ce-80b6-466a-e841-c92042371b08"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "BACKBONE = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BACKBONE)\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgiWsUgWCE-n",
        "outputId": "ec739231-93e5-41e4-a5ef-311f60a8ef6d"
      },
      "source": [
        "special_tokens_dict = {\n",
        "        \"additional_special_tokens\": [\n",
        "            'name:', 'poem:start', 'poem:end'\n",
        "        ]\n",
        "    }\n",
        "tokenizer.add_special_tokens(special_tokens_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCh_-FfuCT3A",
        "outputId": "b7b03a08-cab7-4eb9-c7d1-71b7c0f8d881"
      },
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'additional_special_tokens': \"['name:', 'poem:start', 'poem:end']\",\n",
              " 'bos_token': '<|endoftext|>',\n",
              " 'eos_token': '<|endoftext|>',\n",
              " 'unk_token': '<|endoftext|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9lHS0mIMak4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afff8feb-d0ad-45e8-ef45-2cf2365c27d9"
      },
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=50)\n",
        "     \n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=50)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset('/content/test.txt','/content/test.txt',tokenizer)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hhmbT2ModI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332eff4e-ee5b-4656-b03a-6b0b37b32741"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "BACKBONE = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "model = AutoModelWithLMHead.from_pretrained(BACKBONE)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pushn\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=10, # number of training epochs\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
        "    eval_steps = 100, # Number of update steps between two evaluations.\n",
        "    save_steps=1000, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    fp16_opt_level = 'O4',\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    \n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQh0jlTqR6-"
      },
      "source": [
        "# Train and save the model\n",
        "\n",
        "To train the model we can simply run `Trainer.train()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjIzSWPTKzBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0c295536-15fc-43fa-a298-cef1c983eff1"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         )\n\u001b[1;32m    903\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m                 )\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.60 GiB already allocated; 11.75 MiB free; 13.69 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZrbRlWjgVNJ",
        "outputId": "b2483b1a-4f13-4219-df59-7b039a386f74"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 14 08:51:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W /  70W |  15098MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXqTv8kqxpJ"
      },
      "source": [
        "After training is done you can save the model by calling `save_model()`. This will save the trained model to our `output_dir` from our `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5quyGeMNdjE"
      },
      "source": [
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqHL1goTJqGy",
        "outputId": "07f3ad05-9563-42da-b88b-edaafa210b95"
      },
      "source": [
        "tokenizer.save_pretrained('/content/pushn')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/pushn/tokenizer_config.json',\n",
              " '/content/pushn/special_tokens_map.json',\n",
              " '/content/pushn/vocab.json',\n",
              " '/content/pushn/merges.txt',\n",
              " '/content/pushn/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdYAXJZCNDRT"
      },
      "source": [
        "#Simple api example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n76qIviP0sz"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.middleware import Middleware\n",
        "from starlette.middleware.cors import CORSMiddleware\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "BACKBONE = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "path_model = \"content/pushn\"\n",
        "device='cuda'\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "\n",
        "special_tokens_dict = {\n",
        "        \"additional_special_tokens\": [\n",
        "            'name:', 'poem:start', 'poem:end'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "tok = GPT2Tokenizer.from_pretrained(BACKBONE)\n",
        "tok.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(path_model).to(device)\n",
        "\n",
        "def push(name,lenght=200,temp=1.1,num=2):\n",
        "  \n",
        "  repetition_penalty = 2.6\n",
        "  temperature = temp\n",
        "  top_k =4 \n",
        "\n",
        "  inpt = tok.encode('name:'+name+'poem:start', return_tensors=\"pt\").to(device)\n",
        "\n",
        "  max_length=lenght\n",
        "\n",
        "  out = model.generate(inpt,max_length= max_length,\n",
        "                       do_sample=True, top_k=5000, top_p=0.95, \n",
        "                       temperature=temperature,\n",
        "                       stop_token='poem:end')\n",
        "  decoded = tok.decode(out[0]).replace('>','').replace('<','')\n",
        "  return decoded\n",
        "\n",
        "\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "@app.get(\"/\")\n",
        "def main():\n",
        "    return \"go to /stih/?name=пидр\"\n",
        "\n",
        "\n",
        "\n",
        "@app.get('/push/')\n",
        "def detect_spam_query(name: str, temp: float, length:int):\n",
        "  \n",
        "  return {\"text\":push(text,temp=temp)}\n",
        "\n",
        "import nest_asyncio\n",
        "#from pyngrok import ngrok\n",
        "import uvicorn\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=5001,timeout_keep_alive=10000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew3SOmO3PaQc",
        "outputId": "1cb5e229-6aec-4f66-c017-bc2502f9bc30"
      },
      "source": [
        "print(push(name='Памятник'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: Памятник poem:start  у Бородина,[70 - На полотне, снятом по берлинскому приказу, изображен местный царь, которому выпадает в удел Жермий Тар.Гражданская война, глава I.]\n",
            "\n",
            "\t\tВенок мраморных новых богачей\n",
            "\t\tИз тусклой кучки блестящих ассигнаций.\n",
            "\t\tИ царевич в ленте боярской\n",
            "\t\tЭлладиеву ударя семя,\n",
            "\t\tСо скипетром лавровым\n",
            "\t\tНе дает созреть царствам терпенью.\n",
            "\t\tСвой царизм на деревьях берет,\n",
            "\t\tМечте равнодушно рад —\n",
            "\t\tНедруги царя своевольного,\n",
            "\t\tВсяк свободный миг превраща,\n",
            "\t\tТебя с улыбкой готовы принять,\n",
            "\t\tДо самой вершины тебя не отважишь.\n",
            "\t\tКак мило стяжать венцы – скипетр, скипетр императора!\n",
            "\t\tЦ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO3PFdC0TQE_",
        "outputId": "09990bdc-d8e2-4873-b5f0-0b8757c75843"
      },
      "source": [
        "!zip -r stihi.zip /content/pushn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/pushn/ (stored 0%)\n",
            "  adding: content/pushn/vocab.json (deflated 74%)\n",
            "  adding: content/pushn/config.json (deflated 48%)\n",
            "  adding: content/pushn/added_tokens.json (deflated 26%)\n",
            "  adding: content/pushn/training_args.bin (deflated 46%)\n",
            "  adding: content/pushn/tokenizer_config.json (deflated 66%)\n",
            "  adding: content/pushn/pytorch_model.bin (deflated 16%)\n",
            "  adding: content/pushn/special_tokens_map.json (deflated 68%)\n",
            "  adding: content/pushn/merges.txt (deflated 76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeqzCrHTUY0d",
        "outputId": "667225ce-1e9c-48f8-8202-cde92aa4b2be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}